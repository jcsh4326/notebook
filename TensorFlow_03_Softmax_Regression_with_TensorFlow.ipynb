{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_03_Softmax_Regression_with_TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "veO0U4jpGJYL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcsh4326/notebook/blob/master/TensorFlow_03_Softmax_Regression_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1Cq-Is0GuEy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "cite [Stanford UFLDL Tutorial](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n",
        "\n",
        "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. \n"
      ]
    },
    {
      "metadata": {
        "id": "veO0U4jpGJYL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## logistic regression\n",
        "In logistic regression we assumed that the labels were binary: $y^{(i)} \\in \\{0, 1\\}$. We used such a classifer to distinguish between two kinds of hand-written digits.\n",
        "\n",
        "Softmax regression allows us to handle $y^{(i)} \\in \\{i,...,K\\}$  where K is the number of classes, and so the label $y$ can take on $K$ different values.\n",
        "\n",
        "Recall that in logistic regression, we had a training set $\\{(x^{(1)}, y^{(1)}),..., (x^{(m)}, y^{(m)})\\}$ of $m$ labeled examples, where the input features are $x^{(i)} \\in R^n$. With logistic regression, we were in the binary classification setting, so the labels were $y^{(i)} \\in \\{0, 1\\}$.Our hypothesis took the form:\n",
        "$$ h_θ(x) = \\frac{ 1 }{ 1 + exp(-θ^Tx) } $$\n",
        "\n",
        "<sub>FYI. $T$ for matrix transpose, such as `\\top, \\mathsf{T}, and \\intercal`</sub>\n",
        "\n",
        "and the model parameters θ where trained to minimize the cost function(sometimes called loss function)\n",
        "$$ J(θ) = -[\\sum_{i=0}^m{y^{(i)}logh_θ(x^{(i)})}+(1 - y^{(i)}log(1 - h_θ(x^{(i)})))] $$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AylSJEC-H8hj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FYI. \n",
        "A **logistic function** or **logistic curve** is a common \"S\" shape (sigmoid curve), with equation:\n",
        "$$f(x) = \\frac{L}{1+exp^{-k(x-x_0)}}$$\n",
        "The standard logistic function is the logistic function with parameters (k = 1, x0 = 0, L = 1)"
      ]
    },
    {
      "metadata": {
        "id": "5IWQ2iWqGp6d",
        "colab_type": "code",
        "outputId": "6a6c2769-6de5-4235-8fc8-c694f7c19368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "cell_type": "code",
      "source": [
        "#@title sigmoid function\n",
        "#@markdown sigmoid function is a special case of the Logistic function when L=1, k=1, x0=0 <br> p(x) = 1/(1 + e^(-x))\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "plt.title(\"sigmoid function\")\n",
        "plt.xlabel(\"this is x axis\",fontsize=15,color='b')\n",
        "plt.ylabel(\"this is y axis\",fontsize=15,color='g')\n",
        "x = np.arange(-6.0,6,0.001)\n",
        "plt.plot(x,sigmoid(x),'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFrCAYAAADFOmBlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4aa4aa84a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CO9rJjGjGC1w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## softmax regression\n",
        "In softmax regression we had a training set $\\{(x^{(1)}, y^{(1)}),..., (x^{(m)}, y^{(m)})\\}$,  $y_{(i)} \\in \\{1,2,...,K\\}$. Given a test input $x$, we want our hypothesis to estimate the probability that $P(y = k|x)$ for each value of $k = 1,...,K$. I.e., we want to estimate the probability of the class label taking on each of the $K$ different possible values. Thus, our hypothesis will output a $K-$dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities.  Concretely, our hypothesis $$h_θ(x) = \\begin{pmatrix} P(y = 1|x;θ) \\\\ P(y = 2|x;θ) \\\\\\vdots \\\\ P(y = K|x;θ) \\end{pmatrix} = \\frac{1}{\\sum_{j=1}^K exp(θ^{(j)T}x)}\\begin{pmatrix}exp(θ^{(1)T}x)\\\\exp(θ^{(2)T}x)\\\\ \\vdots \\\\exp(θ^{(K)T}x) \\end{pmatrix}$$\n",
        "\n",
        "Here $θ^{(1)}, θ^{(2)}, \\cdots, θ^{(K)} \\in R^n$ are the parameters of our model. Notice that the term $\\frac{1}{\\sum_{j=1}^K exp(θ^{(j)T}x)}$ normallizes the distribution, so that it sums to one."
      ]
    },
    {
      "metadata": {
        "id": "T9dECGYqMKdh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## cost function\n",
        "In the equation below, $1\\{\\cdot\\}$ is the \"indicator function\", so that $1\\{a true statement\\} = 1$, and $1\\{a false statement\\} = 0$.\n",
        "In the equation \n",
        "###  logistic regression\n",
        "\n",
        "$$J(θ)=-\\lbrack\\sum_{i=1}^m(1-y^{(i)})log(1-h_θ(x^{(i)}))+y^{(i)}logh_θ(x^{(i)})\\rbrack=-[\\sum_{i=1}^m\\sum_{k=0}^11\\{y^{(i)}=k\\}logP(y^{(i)}=k|x^{(i)};θ)]$$\n",
        "\n",
        "in the equation, $k$ is the class labels, $0 or 1$, $m$ is the labeld examples number ,  from $1$ to $m$.\n",
        "\n",
        "Generalizing the logistic regression cost function, our cost function will be:\n",
        "$$J(θ)=-[\\sum_{(i=1)}^m\\sum_{(k=1)}^K1\\{y^{(i)=k}\\}log\\frac{exp(θ^{(k)T}x^{(i)})}{\\sum_{j=1}^Kexp(θ^{(j)T}x^{(i)})}]$$\n",
        "\n",
        "### softmax regression\n",
        "The softmax regression cost function is similar, except that we now sum over the $K$ different possible values of the class label. Note also that in softmax regression, we have that\n",
        "$$P(y^{(i)} = k|x^{(i)};θ) = \\frac{exp(θ^{(k)T}x^{(i)})}{\\sum_{j=1}^Kexp(θ^{(j)T}x^{(i)})}$$\n",
        "\n",
        "For solving the minimum of $J(θ)$, we resort to an iterative optimization algorithm, such as L-BFGS, gradient descent.\n",
        "\n",
        "Taking derivatives,  one can show that the gradient is:\n",
        "$$\\nabla_{\\theta^{(k)}}J(\\theta) = - \\sum_{i=1}^M[x^{(i)}(1\\{y^{(i)}=k\\}-P(y{(i)}=k|x^{(i)};\\theta))]$$\n",
        "\n",
        "In particular, $\\nabla_{\\theta^{(k)}}J(\\theta)$ is itself a vector, so that its $l$-th element is $\\frac{\\partial J(\\theta)}{\\partial\\theta_{lk}}$ the partial derivative of $J(\\theta)$ with respect of the $l$-th element of $\\theta^{(k)}$.\n",
        "\n",
        "In the standard implementation of gradient descent, each iteration requires the following updates:\n",
        "$$\\theta_j := \\theta_j - \\alpha\\nabla_{\\theta_j}J(\\theta)(j = 1,...,k)$$\n",
        "\n",
        "## weight decay\n",
        "Softmax regression has a \"redundant\" set of parameters.\n",
        "\n",
        "If we subtract some fixed vector $\\psi$ from out parameter vectors $\\theta^{(j)}$, so that every $\\theta^{(j)}$ is now replaced with $\\theta^{(j)} - \\psi$ (for every $j = 1,...,k$). So the class label probabilities as \n",
        "$$P(y^{(i)} = k|x^{(i);\\theta}) =  \\frac{exp((θ^{(k)}-\\psi)^Tx^{(i)})}{\\sum_{j=1}^Kexp((θ^{(j)}-\\psi)^Tx^{(i)})}=\\frac{exp(\\theta^{(k)T}x^{(i)})exp(-\\psi^Tx^{(i)})}{\\sum_{j=1}^Kexp(\\theta^{(j)T}x^{(i)})exp(-\\psi^Tx^{(i)})}= \\frac{exp(θ^{(k)T}x^{(i)})}{\\sum_{j=1}^Kexp(θ^{(j)T}x^{(i)})}$$\n",
        "\n",
        "As the equation shows, there are multiple parameter settings that give rise to exactly the same hypothesis function $h_\\theta$ mapping from inputs $x$ to the predictions. Thus, the minimizer of $J(\\theta)$ is not unique.\n",
        "\n",
        "$J(\\theta)$ is still convex, and thus gradient descent will not run into local optima problems, but the Hessian is singular/non-invertible, which causes a straightforward implementation of Newton’s method to run into numerical problems.\n",
        "\n",
        "for optimizing the clear and definite parameters, we remain all the parameters instead of setting one of the parameters to 0 by changing the cost function with \"weight decay item\".\n",
        "\n",
        "A weight decay item $\\frac{ \\lambda }{2}\\sum_{i=1}^k\\sum_{j=0}^m\\theta_{ij}^2$ penalizes large values of the parameters.\n",
        "\n",
        "$$J(θ)=-[\\sum_{(i=1)}^m\\sum_{(k=1)}^K1\\{y^{(i)=k}\\}log\\frac{exp(θ^{(k)T}x^{(i)})}{\\sum_{j=1}^Kexp(θ^{(j)T}x^{(i)})}] + \\frac{ \\lambda }{2}\\sum_{i=1}^k\\sum_{j=0}^m\\theta_{ij}^2$$\n",
        "\n",
        "The cost function now is strictly convex, and is guaranteed to have a unique solution.\n",
        "\n",
        "To apply an optimization algorithm, we also need the derivative of this new definition of $J(\\theta)$, one can show that the derivative is:\n",
        " $$\\nabla_{\\theta^{(k)}}J(\\theta) = - \\sum_{i=1}^M[x^{(i)}(1\\{y^{(i)}=k\\}-P(y{(i)}=k|x^{(i)};\\theta))] + \\lambda\\theta_k $$\n"
      ]
    },
    {
      "metadata": {
        "id": "aXLEcXCEMAD0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implement\n"
      ]
    },
    {
      "metadata": {
        "id": "tvuh-z8HCZ98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        " Below we will implement a softmax regression algorithm with TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "XKSNQluw7A7H",
        "colab_type": "code",
        "outputId": "592c3cf9-4eca-41e7-80ba-badb1ae71e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "#@title import\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-1619da060e78>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hETUPiZ7TmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title define placeholder for inputs to algorithm\n",
        "inputs = tf.placeholder(tf.float32, [None, None])\n",
        "labels = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6oMQkgh8WXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title add layer\n",
        "def add_layer(inputs, w_dimension):\n",
        "  # 10 class labels\n",
        "  W = tf.Variable(tf.zeros([w_dimension, 10]))\n",
        "  biases = tf.Variable(tf.zeros([10]))\n",
        "  outputs = tf.nn.softmax(tf.matmul(inputs, W) + biases)\n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IF1kMbKo-HUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title define loss function/coss function\n",
        "\n",
        "prediction = add_layer(inputs, w_dimension=784)\n",
        "loss = tf.reduce_sum(labels*tf.log(prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6QCeUI5-oZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title train\n",
        "l_r = 0.01 # learning rate\n",
        "train_step = tf.train.GradientDescentOptimizer(l_r).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_xcBAeC_yYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title init\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ROiy-vFg_44o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title train data\n",
        "itera = 1000\n",
        "for i in range(itera):\n",
        "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "  sess.run(train_step, feed_dict={inputs: batch_xs, labels: batch_ys})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VtOeXg4ICMYp",
        "colab_type": "code",
        "outputId": "f7140ce6-f66e-4029-f172-7871bcfe7192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "print(sess.run(accuracy, feed_dict={inputs: mnist.test.images, labels: mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.098\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}